---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a third-year Ph.D. candidate at the Institute of Artificial Intelligence and Robotics ([IAIR](https://iair.xjtu.edu.cn/index.htm), Xi'an Jiaotong University (XJTU), where I pursue my doctoral studies under the supervision of Prof.[Le Wang](https://gr.xjtu.edu.cn/web/lewang). I also collaborate closely with Prof.[Sanping Zhou](https://gr.xjtu.edu.cn/web/spzhou), Prof.[Gang Hua](https://www.ganghua.org/) and Prof.[Wei Tang](https://www.cs.uic.edu/~tangw/).
 
I am currently a visiting Ph.D. student with the Multimedia and Human Understanding Group (MHUG) at the University of Trento, under the supervision of [Nicu Sebe](https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en).




# üî• News {#news}
- *2025.10*: &nbsp;üéâüéâ One paper is accepted by AAAI 2026.
- *2025.07*: &nbsp;üéâüéâ One paper is accepted by ACM MM 2025.
- *2025.05*: &nbsp;üéâüéâ Two paper is accepted by ICCV 2025.
- *2025.03*: &nbsp;üéâüéâ Two paper is accepted by CVPR 2025.
- *2024.10*: &nbsp;üéâüéâ One paper is accepted by AAAI 2025.
- *2024.10*: &nbsp;üéâüéâ One paper is accepted by ACM MM 2024.
- *2024.09*: &nbsp;üéâüéâ One paper is accepted by NIPS 2024.
- *2024.02*: &nbsp;üéâüéâ One paper is accepted by TIP.
- *2023.10*: &nbsp;üéâüéâ One paper is accepted by AAAI 2024.
- *2022.10*: &nbsp;üéâüéâ One paper is accepted by ACM MM 2022.
- *2022.10*: &nbsp;üéâüéâ One paper is accepted by TCSVT.




# üìù Selected Publications {#selectedpublications}
<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/nips2024.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Referencing Where to Focus: Improving Visual Grounding with Referential Query</h3>
    <div class="authors">
      <strong>Y Wang</strong>, Z Tian, Q Guo, Z Qin, S Zhou, M Yang, L Wang
    </div>
    <div class="venue">NIPS2024</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2412.19155"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/LiJiaBei-7/RefFormer"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/aaai2025.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>RefDetector: A Simple yet Effective Matching-based Method for Referring Expression Comprehension</h3>
    <div class="authors">
      <strong>Y Wang</strong>, Z Tian, Z Qin, S Zhou, L Wang
    </div>
    <div class="venue">AAAI2025</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32866/35021"><i class="fas fa-file-alt"></i> Paper</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/mm2022.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Cross-lingual cross-modal retrieval with noise-robust learning</h3>
    <div class="authors">
      <strong>Y Wang</strong>,  J Dong, T Liang, M Zhang, R Cai, X Wang
    </div>
    <div class="venue">ACM MM2022</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2208.12526"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/LiJiaBei-7/nrccr"><i class="fab fa-github"></i> Code</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/mm2024.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Multimodal llm enhanced cross-lingual cross-modal retrieval</h3>
    <div class="authors">
      <strong>Y Wang</strong>,  L Wang, Q Zhou, Z Wang, H Li, G Hua, W Tang
    </div>
    <div class="venue">ACM MM2024</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2409.19961"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/LiJiaBei-7/leccr"><i class="fab fa-github"></i> Code</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/aaai2024.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual Knowledge Transfer</h3>
    <div class="authors">
      <strong>Y Wang</strong>, F Wang, J Dong, H Luo
    </div>
    <div class="venue">AAAI24</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2312.08984"><i class="fas fa-file-alt"></i> Paper</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/tip2024.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Dual-view curricular optimal transport for cross-lingual cross-modal retrieval</h3>
    <div class="authors">
      <strong>Y Wang</strong>, S Wang, H Luo, J Dong, F Wang, M Han, X Wang, M Wang
    </div>
    <div class="venue">TIP2024</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2309.05451"><i class="fas fa-file-alt"></i> Paper</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>
</div>

</div>


<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/tcsvt2022.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Reading-strategy inspired visual representation learning for text-to-video retrieval</h3>
    <div class="authors">
      J Dong#, <strong>Y Wang</strong>#, X Chen, X Qu, X Li, Y He, X Wang
    </div>
    <div class="venue">TCSVT2022</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2201.09168"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/LiJiaBei-7/rivrl"><i class="fab fa-github"></i> Code</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <!-- <div class="badge pulse-accent">Visual Grounding</div> -->
    <img src='images/arxiv2025.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>From mapping to composing: A two-stage framework for zero-shot composed image retrieval</h3>
    <div class="authors">
      <strong>Y Wang</strong>, Z Tian, Q Guo, Z Qin, S Zhou, M Yang, L Wang
    </div>
    <div class="venue">Arxiv2025</div>
    <div class="links">
      <!-- <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a> -->
      <a href="https://arxiv.org/abs/2504.17990"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/LiJiaBei-7/TSCIR"><i class="fab fa-github"></i> Code</a>
      <!-- <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a> -->
    </div>
  </div>
</div>

# Publications  {#publications}
- Reading-strategy inspired visual representation learning for text-to-video retrieval, TCSVT 2022  
  J Dong^#, **Y Wang**^#, X Chen, X Qu, X Li, Y He, X Wang
  
  
- Cross-lingual cross-modal retrieval with noise-robust learning, ACM MM 2022  
  **Y Wang**, J Dong, T Liang, M Zhang, R Cai, X Wang


- Multimodal llm enhanced cross-lingual cross-modal retrieval, ACM MM 2024  
  **Y Wang**, L Wang, Q Zhou, Z Wang, H Li, G Hua, W Tang  


- CL2CM: Improving Cross-Lingual Cross-Modal Retrieval via Cross-Lingual Knowledge Transfer, AAAI 2024  
  **Y Wang**, F Wang, J Dong, H Luo
  

- Dual-view curricular optimal transport for cross-lingual cross-modal retrieval, TIP 2024  
  **Y Wang**, S Wang, H Luo, J Dong, F Wang, M Han, X Wang, M Wang
  

- Referencing Where to Focus: Improving Visual Grounding with Referential Query, NeurIPS 2024    
  **Y Wang**, Z Tian, Q Guo, Z Qin, S Zhou, M Yang, L Wang

- RefDetector: A Simple yet Effective Matching-based Method for Referring Expression Comprehension, AAAI 2025      
  **Y Wang**, Z Tian, Z Qin, S Zhou, L Wang

- From mapping to composing: A two-stage framework for zero-shot composed image retrieval, arxiv 2025    
  **Y Wang**, Z Tian, Q Guo, Z Qin, S Zhou, M Yang, L Wang

- Moment Quantization for Video Temporal Grounding, ICCV 2025      
  X Sun, L Wang, S Zhou, L Shi, K Xia, M Liu, **Y Wang**, G Hua

- Towards precise embodied dialogue localization via causality guided diffusion, CVPR 2025      
  H Wang, L Wang, S Zhou, J Tian, Z Qin, **Y Wang**, G Hua, W Tang  

- HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs, AAAI 2026    
  Z Qin, R Zheng, **Y Wang**, T Li, Y Yuan, J Chen, L Wang


- Avf-mae++: Scaling affective video facial masked autoencoders via efficient audio-visual self-supervised learning, CVPR 2025  
  X Wu, H Sun, **Y Wang**, J Nie, J Zhang, Y Wang, J Xue, L He  


- Diversifying query: Region-guided transformer for temporal sentence grounding, AAAI 2025    
  X Sun, L Shi, L Wang, S Zhou, K Xia, **Y Wang**, G Hua  

- Cross-lingual cross-modal retrieval with noise-robust fine-tuning, IEEE TKDE 2024   
  R Cai, J Dong, T Liang, Y Liang, **Y Wang**, X Yang, X Wang, M Wang  

- Versatile multimodal controls for expressive talking human animation, ACM MM 2025    
  Z Qin, R Zheng, **Y Wang**, T Li, Z Zhu, S Zhou, M Yang, L Wang  

- FeatInter: exploring fine-grained object features for video-text retrieval, Neurocomputing 2022   
  B Liu, Q Zheng, **Y Wang**, M Zhang, J Dong, X Wang  

- Progressive localization networks for language-based moment localization, ACM TOMM 2023  
  Q Zheng, J Dong, X Qu, X Yang, **Y Wang**, P Zhou, B Liu, X Wang  

- Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion, submitted to TCSVT    
  Z Qin, **Y Wang**, M Yang, S Zhou, M Yang, L Wang, 2025

- RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation, submitted to TCSVT    
  Z Qin, L Wang, **Y Wang**, S Zhou, G Hua, W Tang


# üìñ Educations {#educations}
- *2025.09 - now*, Visiting Ph.D. student, Artificial Intelligence, University of Trento.
- *2023.09 - now*, Ph.D. student, Control Science and Engineering, Xi'an Jiaotong University. 


# üíª Internships {#internships}
- *2024.05 - 2025.07*, Ant Group, Bailing Large Model TeamÔºå Research Intern


# üéñ Honors and Scholarships {#honors-and-scholarships}
- **National Scholarship**, 2025
- **National Scholarship**, 2022 
- **Yidong Scholarship**, 2024
- **Bronze Award of China International College Student Innovation Competition**, 2023
- **Outstanding Graduates in Zhejiang Province**, 2023
- **Gold Award of the 18th "Challenge Cup" Industrial and Commercial Bank of China College Students Extracurricular Academic Science and Technology Works Competition in Zhejiang Provinc**, 2023
- **The ninth "Internet +" Silver Award in Zhejiang Province**, 2023


# Services {#services}
Reviewer for CVPR, ICCV, ICML, ECCV, ICLR, NIPS, ACM MM, AAAI, TIP, TMM, PR, etc.

